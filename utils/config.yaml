# Configuration for the LLM training

# Data parameters
dataset_path: data/input.txt
train_ratio: 0.8
val_ratio: 0.1
test_ratio: 0.1
block_size: 8
batch_size: 32

# Model parameters
vocab_size: 65 # Example value, adjust as needed.
embedding_dim: 128
hidden_dim: 256
num_layers: 2
dropout: 0.2
init_type: "xavier_uniform" # Weight initialization

# Training parameters
learning_rate: 0.001
epochs: 10
optimizer: AdamW # or SGD, Adam, etc.
checkpoint_path: checkpoints/model.pth
log_file: logs/training.log

# Generation parameters
max_gen_length: 200
temperature: 1.0
top_k: 50
top_p: 0.9
