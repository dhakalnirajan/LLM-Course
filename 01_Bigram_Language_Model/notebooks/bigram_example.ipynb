{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import bigram\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "with open('../data/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Train Model ---\n",
    "if os.path.exists('../model.pth'):\n",
    "    # Load the model\n",
    "    print(\"Loading the pre-trained model\")\n",
    "    model, encode, decode, block_size = bigram.load_bigram_model('../model.pth')\n",
    "\n",
    "else:\n",
    "    model, encode, decode = bigram.train_bigram(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Generate text from the model ---\n",
    "\n",
    "# Generate text starting from an empty context\n",
    "print(\"Generating text starting from an empty context\")\n",
    "print(bigram.generate_text(model, encode, decode, max_new_tokens=200))\n",
    "\n",
    "\n",
    "print(\"Generating text starting from 'T'\")\n",
    "generated_text = bigram.generate_text(model, encode, decode, start_str=\"T\", max_new_tokens=200)\n",
    "print(generated_text)\n",
    "\n",
    "print(\"Generating text starting from 'This'\")\n",
    "generated_text = bigram.generate_text(model, encode, decode, start_str=\"This\", max_new_tokens=200)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize the embedding table (Optional, but insightful) ---\n",
    "# This part is useful for visualizing what the model has learned,\n",
    "# although with the bigram model, it's just a direct mapping.\n",
    "try: #added error handling\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(model.token_embedding_table.weight.detach().numpy())\n",
    "    plt.title(\"Token Embedding Table\")\n",
    "    plt.xlabel(\"Token Index (Next Token)\")\n",
    "    plt.ylabel(\"Token Index (Current Token)\")\n",
    "    plt.colorbar(label=\"Logit Value\")\n",
    "\n",
    "    # Add labels to the axes (if the vocabulary is small enough)\n",
    "    chars = sorted(list(set(text)))\n",
    "    itos = {i: ch for i, ch in enumerate(chars)}\n",
    "    if len(chars) < 50:  # Avoid cluttering the plot if vocab is too large\n",
    "        plt.xticks(range(len(chars)), [itos[i] for i in range(len(chars))])\n",
    "        plt.yticks(range(len(chars)), [itos[i] for i in range(len(chars))])\n",
    "    plt.show()\n",
    "except ValueError as e:\n",
    "    print(f\"An error occurred during visualization: {e}\")\n",
    "    print(\"This may happen if Matplotlib is not properly configured, or if you're running in an environment without a display.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
