{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports ---\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the utils directory to the Python path\n",
    "# We go up to the project root, then down into utils\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'utils'))\n",
    "\n",
    "# Import the utility modules\n",
    "import data_utils\n",
    "import model_utils\n",
    "import eval_utils\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
    "import bigram  # Import the bigram module from the src directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load and Prepare Data ---\n",
    "# 1. Load the text data\n",
    "data_filepath = os.path.join(os.path.dirname(__file__), '..', 'data', 'input.txt')\n",
    "text = data_utils.load_text_data(data_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the vocabulary\n",
    "stoi, itos = data_utils.create_vocabulary(text)\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Encode the text\n",
    "encoded_text = data_utils.encode_text(text, stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Split the data (optional, but good practice to see performance on unseen data)\n",
    "train_data, val_data, test_data = data_utils.split_data(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Hyperparameters (you could also load these from a config file)\n",
    "batch_size = 32\n",
    "block_size = 8 #context length\n",
    "epochs = 10 #training epochs\n",
    "learning_rate = 1e-2\n",
    "model_save_path = os.path.join(os.path.dirname(__file__), '..', 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Model Training or Loading ---\n",
    "if os.path.exists(model_save_path):\n",
    "    # Load the pre-trained model\n",
    "    print(\"Loading the pre-trained model...\")\n",
    "    checkpoint = model_utils.load_checkpoint(model_save_path, bigram.BigramLanguageModel, vocab_size=vocab_size) #Pass vocab_size to the model.\n",
    "    model = checkpoint['model']\n",
    "    # Note: stoi and itos are not strictly part of the model, so load them separately\n",
    "    # This is a design choice; you *could* include them in the checkpoint.\n",
    "    loaded_stoi = checkpoint['stoi']\n",
    "    loaded_itos = checkpoint['itos']\n",
    "\n",
    "    # Sanity check: Ensure loaded vocab matches current vocab\n",
    "    assert stoi == loaded_stoi, \"Loaded stoi does not match current stoi!\"\n",
    "    assert itos == loaded_itos, \"Loaded itos does not match current itos!\"\n",
    "\n",
    "else:\n",
    "    print(\"Training a new model...\")\n",
    "    # 1. Create DataLoaders\n",
    "    train_loader = data_utils.get_data_loader(train_data, batch_size=batch_size, block_size=block_size)\n",
    "    val_loader = data_utils.get_data_loader(val_data, batch_size=batch_size, block_size=block_size) # Use validation data\n",
    "\n",
    "    # 2. Instantiate the model and optimizer\n",
    "    model = bigram.BigramLanguageModel(vocab_size)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # 3. Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train() #set to train mode\n",
    "        for xb, yb in train_loader:\n",
    "            # Forward pass and loss\n",
    "            logits, loss = model(xb, yb)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % (epochs//5) == 0 or epoch == 0:\n",
    "             # --- Validation Loss (inside the training loop) ---\n",
    "            model.eval()  # Set the model to evaluation mode\n",
    "            with torch.no_grad():\n",
    "                val_loss = 0\n",
    "                for v_xb, v_yb in val_loader:\n",
    "                    v_logits, v_loss = model(v_xb, v_yb)\n",
    "                    val_loss+= v_loss.item()\n",
    "                val_loss /= len(val_loader)  # Get the average validation loss\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # 4. Save the trained model\n",
    "    model_utils.save_checkpoint(model, optimizer, epochs, model_save_path, stoi=stoi, itos=itos, vocab_size=vocab_size, block_size=block_size) # Include stoi, itos in the checkpoint\n",
    "    print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Text Generation ---\n",
    "print(\"\\nGenerating text:\")\n",
    "generated_text = eval_utils.generate_text(model, stoi, itos, max_length=200, device=model_utils.get_device())\n",
    "print(generated_text)\n",
    "\n",
    "print(\"\\nGenerating text starting from 'T':\")\n",
    "start_tokens = data_utils.encode_text(\"T\", stoi)\n",
    "generated_text = eval_utils.generate_text(model, stoi, itos, start_tokens=start_tokens, max_length=200, device=model_utils.get_device())\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- (Optional) Visualization of Embedding Table ---\n",
    "# This part is useful for visualizing what the model has learned,\n",
    "# although with the bigram model, it's just a direct mapping.\n",
    "try:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(model.token_embedding_table.weight.detach().cpu().numpy())  # Move to CPU for plotting\n",
    "    plt.title(\"Token Embedding Table\")\n",
    "    plt.xlabel(\"Token Index (Next Token)\")\n",
    "    plt.ylabel(\"Token Index (Current Token)\")\n",
    "    plt.colorbar(label=\"Logit Value\")\n",
    "\n",
    "    # Add labels to the axes (if the vocabulary is small enough)\n",
    "    if vocab_size < 50:\n",
    "        plt.xticks(range(vocab_size), [itos[i] for i in range(vocab_size)])\n",
    "        plt.yticks(range(vocab_size), [itos[i] for i in range(vocab_size)])\n",
    "    plt.show()\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"An error occurred during visualization: {e}\")\n",
    "    print(\"This may happen if Matplotlib is not properly configured.\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Runtime error during visualization: {e}\") #Handle runtime errors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
