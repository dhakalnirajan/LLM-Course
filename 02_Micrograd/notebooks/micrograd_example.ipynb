{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import graphviz  # For visualization (optional, but very helpful)\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
    "import engine\n",
    "from engine import Value\n",
    "import nn\n",
    "from nn import Module, Neuron, Layer, MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Visualization Function (Optional, but Recommended) ---\n",
    "def trace(root):\n",
    "    \"\"\"\n",
    "    Builds a set of all nodes and edges in a graph, starting from the given root.\n",
    "    This is for visualization using graphviz.\n",
    "    \"\"\"\n",
    "    nodes, edges = set(), set()\n",
    "    def build(v):\n",
    "        if v not in nodes:\n",
    "            nodes.add(v)\n",
    "            for child in v._prev:\n",
    "                edges.add((child, v))\n",
    "                build(child)\n",
    "    build(root)\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def draw_dot(root, format='svg', rankdir='LR'):\n",
    "    \"\"\"\n",
    "    Generates a Graphviz visualization of the computational graph.\n",
    "    Requires the graphviz library.\n",
    "    \"\"\"\n",
    "    assert rankdir in ['LR', 'TB']\n",
    "    nodes, edges = trace(root)\n",
    "    dot = graphviz.Digraph(format=format, graph_attr={'rankdir': rankdir})\n",
    "\n",
    "    for n in nodes:\n",
    "        # For each node, create a rectangular ('record') node for it\n",
    "        dot.node(name=str(id(n)), label = \"{ %s | data %.4f | grad %.4f }\" % (n.label, n.data, n.grad), shape='record')\n",
    "        if n._op:\n",
    "            # If this Value is the result of some operation, create an operation node for it,\n",
    "            # and connect Value nodes to this op node.\n",
    "            dot.node(name=str(id(n)) + n._op, label=n._op)\n",
    "            dot.edge(str(id(n)) + n._op, str(id(n)))\n",
    "\n",
    "    for n1, n2 in edges:\n",
    "        # Connect n1 to the op node of n2\n",
    "        dot.edge(str(id(n1)), str(id(n2)) + n2._op)\n",
    "\n",
    "    return dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example 1: Simple Expression ---\n",
    "print(\"Example 1: Simple Expression\")\n",
    "a = Value(2.0, label='a')\n",
    "b = Value(-3.0, label='b')\n",
    "c = Value(10.0, label='c')\n",
    "e = a * b; e.label = 'e'\n",
    "d = e + c; d.label = 'd'\n",
    "f = Value(-2.0, label='f')\n",
    "L = d * f; L.label = 'L'\n",
    "\n",
    "print(f\"L: {L}\")  # Value(data=-8.0)\n",
    "\n",
    "# Compute gradients\n",
    "L.backward()\n",
    "\n",
    "# Visualize (Optional)\n",
    "# dot = draw_dot(L)\n",
    "# dot.render(directory='notebooks', view=True, filename='example1_graph')  # Saves to a file and opens it\n",
    "#To see the graphviz output, uncomment the previous two lines and make sure graphviz is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example 2: Manual Neuron ---\n",
    "print(\"\\nExample 2: Manual Neuron\")\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "o = n.tanh(); o.label = 'o'\n",
    "\n",
    "print(f\"o: {o}\")  # Value(data=0.7071)\n",
    "\n",
    "# Compute gradients\n",
    "o.backward()\n",
    "\n",
    "# Visualize\n",
    "# dot = draw_dot(o)\n",
    "# dot.render(directory='notebooks', view=True, filename='example2_graph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example 3: Neuron with exp and division ---\n",
    "print(\"\\nExample 3: Neuron with exp and division\")\n",
    "# inputs x1,x2\n",
    "x1 = Value(2.0, label='x1')\n",
    "x2 = Value(0.0, label='x2')\n",
    "# weights w1,w2\n",
    "w1 = Value(-3.0, label='w1')\n",
    "w2 = Value(1.0, label='w2')\n",
    "# bias of the neuron\n",
    "b = Value(6.8813735870195432, label='b')\n",
    "# x1*w1 + x2*w2 + b\n",
    "x1w1 = x1*w1; x1w1.label = 'x1*w1'\n",
    "x2w2 = x2*w2; x2w2.label = 'x2*w2'\n",
    "x1w1x2w2 = x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'\n",
    "n = x1w1x2w2 + b; n.label = 'n'\n",
    "\n",
    "# ----\n",
    "e = (2*n).exp(); e.label='e'\n",
    "o = (e-1)/(e+1)\n",
    "# ----\n",
    "o.label = 'o'\n",
    "o.backward()\n",
    "print(f\"o: {o}\")  # Value(data=0.7071)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example 4: Using the nn module ---\n",
    "print(\"\\nExample 4: Using the nn module\")\n",
    "\n",
    "# Binary classification\n",
    "# Input data\n",
    "xs = [\n",
    "  [2.0, 3.0, -1.0],\n",
    "  [3.0, -1.0, 0.5],\n",
    "  [0.5, 1.0, 1.0],\n",
    "  [1.0, 1.0, -1.0],\n",
    "]\n",
    "ys = [1.0, -1.0, -1.0, 1.0]  # Desired targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create an MLP\n",
    "model = MLP(3, [4, 4, 1])  # 3 inputs, two hidden layers with 4 neurons each, 1 output\n",
    "print(model)\n",
    "print(f\"Number of parameters: {len(model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "n_epochs = 20\n",
    "learning_rate = 0.05 # Start with a reasonable learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for k in range(n_epochs):\n",
    "    # Forward pass\n",
    "    ypred = [model(x) for x in xs]\n",
    "    loss = sum((yout - ygt)**2 for ygt, yout in zip(ys, ypred))\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    for p in model.parameters():\n",
    "        p.data -= learning_rate * p.grad\n",
    "\n",
    "    if (k+1)%(n_epochs//5) == 0 or k == 0:\n",
    "        print(f\"Epoch {k + 1}, Loss: {loss.data}\")\n",
    "\n",
    "print(f\"Predictions: {[yp.data for yp in ypred]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Example 5: Using the nn module, Cross Entropy Loss ---\n",
    "print(\"\\nExample 5: Cross Entropy\")\n",
    "# Example data: 2 samples, 3 classes\n",
    "logits = [\n",
    "    [2.0, -1.0, 3.0], #model output for first input\n",
    "    [0.5, 2.0, -1.5]  #model output for the second input\n",
    "    ]\n",
    "\n",
    "targets = [0,1] #class 0 for first input, class 1 for second input\n",
    "\n",
    "#Convert to Value objects\n",
    "logits_v = [[Value(l) for l in logit] for logit in logits]\n",
    "targets_v = [t for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, targets):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "      logits: A list of lists of Value objects, where each inner list\n",
    "        represents the output logits from the model for a single input.\n",
    "      targets: A list of integers representing the correct class indices.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for i, logit_row in enumerate(logits):\n",
    "        # 1. Calculate softmax probabilities\n",
    "        exp_logits = [l.exp() for l in logit_row]\n",
    "        sum_exp_logits = sum(exp_logits, Value(0.0)) #important to start with Value type\n",
    "        probs = [el / sum_exp_logits for el in exp_logits]\n",
    "\n",
    "        # 2. Calculate the negative log likelihood for the correct class\n",
    "        correct_class_index = targets[i]\n",
    "        nll = -probs[correct_class_index].log()  # We don't have log, you can derive.\n",
    "        losses.append(nll)\n",
    "    return sum(losses, Value(0.0)) / Value(float(len(losses))) #return the mean loss\n",
    "\n",
    "loss = cross_entropy_loss(logits_v, targets_v)\n",
    "print(f\"Cross-entropy loss: {loss.data}\")\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "#Now you can access gradients:\n",
    "# print(logits_v[0][0].grad)\n",
    "# print(logits_v[0][1].grad)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
